{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Instituto Tecnológico de Costa Rica - TEC**\n",
        "\n",
        "***Inteligencia Artificial***\n",
        "\n",
        "*Docente: Kenneth Obando Rodríguez*\n",
        "\n",
        "---\n",
        "# Trabajo Corto 3: Árboles de Decisión\n",
        "---\n",
        "Estudiantes:\n",
        "- Renzo Giuliano Barra Mostajo\n",
        "- Ana María Guevara Roselló\n",
        "- Jonathan Alberto Guzmán Araya\n",
        "\n",
        "Link del Cuaderno (recuerde configurar el acceso a público):\n",
        "\n",
        "    \n",
        "- [Link de su respuesta](https://drive.google.com/file/d/1NDq4IN_-X8_ePfwLxVTYwaqMAfUZ46Pf/view?usp=sharing)\n",
        "\n",
        "    **Nota:** Este trabajo tiene como objetivo promover la comprensión de la materia y su importancia en la elección de algoritmos. Los alumnos deben evitar copiar y pegar directamente información de fuentes externas, y en su lugar, demostrar su propio análisis y comprensión.\n",
        "\n",
        "### Entrega\n",
        "Debe entregar un archivo comprimido por el TecDigital, incluyendo un documento pdf con los resultados de los experimentos y pruebas. La fecha de entrega es el domingo 17 de setiembre, antes de las 10:00pm.   "
      ],
      "metadata": {
        "id": "4eNq4lCSSQWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instrucciones:\n",
        "\n",
        "Las alternativas se rifarán en clase utilizando números aleatorios. Deberá realizar la asignación propuesta. Si realiza ambos ejercicios, recibirá 20 puntos en **la nota porcentual de la actividad**, para aplicar a la totalidad de los puntos extra es necesario que ambas actividades se completen al 100%\n"
      ],
      "metadata": {
        "id": "XmAGdF_-Xhj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actividad - Taller\n",
        "\n",
        "1. Cree una clase nodo con atributos necesarios para un árbol de decisión: feature, umbral, gini, cantidad_muestras, valor, izquierda, derecha\n"
      ],
      "metadata": {
        "id": "onCmI27E3Jit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Step 1: Node Class Creation\n",
        "# This class represents a node in the decision tree.\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, impurity=None, sample_count=None, value=None, left=None, right=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.impurity = impurity\n",
        "        self.sample_count = sample_count\n",
        "        self.value = value\n",
        "        self.left = left\n",
        "        self.right = right"
      ],
      "metadata": {
        "id": "sujTWE1gSTJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find the most common class in a list of labels\n",
        "def most_common_class(y):\n",
        "    class_counts = Counter(y)\n",
        "    most_common = class_counts.most_common(1)[0][0]\n",
        "    return most_common\n",
        "\n",
        "\n",
        "# Function to select the best feature and threshold for splitting\n",
        "def find_best_split(X, y, criterion='gini'):\n",
        "    best_impurity = float('inf')\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "\n",
        "    for feature in range(X.shape[1]):\n",
        "        unique_thresholds = np.unique(X[:, feature])\n",
        "        for threshold in unique_thresholds:\n",
        "            left_indices = X[:, feature] <= threshold\n",
        "            right_indices = X[:, feature] > threshold\n",
        "            impurity = calculate_impurity(\n",
        "                y[left_indices], y[right_indices], criterion)\n",
        "\n",
        "            if impurity < best_impurity:\n",
        "                best_impurity = impurity\n",
        "                best_feature = feature\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature, best_threshold\n",
        "\n",
        "\n",
        "# Impurity Functions (Gini and Entropy)\n",
        "# These functions calculate the impurity of a set of labels.\n",
        "def calculate_impurity(y_left, y_right, criterion='gini'):\n",
        "    if criterion == 'gini':\n",
        "        impurity_left = gini_impurity(y_left)\n",
        "        impurity_right = gini_impurity(y_right)\n",
        "        total_samples = len(y_left) + len(y_right)\n",
        "        weighted_impurity = (len(y_left) / total_samples) * impurity_left + \\\n",
        "            (len(y_right) / total_samples) * impurity_right\n",
        "        return weighted_impurity\n",
        "    elif criterion == 'entropy':\n",
        "        entropy_left = entropy_impurity(y_left)\n",
        "        entropy_right = entropy_impurity(y_right)\n",
        "        total_samples = len(y_left) + len(y_right)\n",
        "        weighted_entropy = (len(y_left) / total_samples) * entropy_left + \\\n",
        "            (len(y_right) / total_samples) * entropy_right\n",
        "        return weighted_entropy\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Invalid criterion. Supported criteria are 'gini' and 'entropy'.\")\n",
        "\n",
        "\n",
        "# Function to calculate the entropy of a set of labels\n",
        "def entropy_impurity(labels):\n",
        "    num_samples = len(labels)\n",
        "    if num_samples == 0:\n",
        "        return 0.0\n",
        "\n",
        "    class_counts = Counter(labels)\n",
        "    impurity = 0.0\n",
        "    for class_count in class_counts.values():\n",
        "        class_probability = class_count / num_samples\n",
        "        impurity -= class_probability * np.log2(class_probability)\n",
        "\n",
        "    return impurity\n",
        "\n",
        "\n",
        "# Function to calculate the Gini index of a set of labels\n",
        "def gini_impurity(labels):\n",
        "    num_samples = len(labels)\n",
        "    if num_samples == 0:\n",
        "        return 0.0\n",
        "    class_counts = Counter(labels)\n",
        "    impurity = 1.0\n",
        "    for class_count in class_counts.values():\n",
        "        class_probability = class_count / num_samples\n",
        "        impurity -= class_probability ** 2\n",
        "\n",
        "    return impurity\n",
        "\n",
        "\n",
        "# Function to split the dataset into left and right subsets\n",
        "def split_dataset(X, y, feature, threshold):\n",
        "    left_indices = X[:, feature] <= threshold\n",
        "    right_indices = X[:, feature] > threshold\n",
        "\n",
        "    X_left = X[left_indices]\n",
        "    y_left = y[left_indices]\n",
        "\n",
        "    X_right = X[right_indices]\n",
        "    y_right = y[right_indices]\n",
        "\n",
        "    return X_left, y_left, X_right, y_right"
      ],
      "metadata": {
        "id": "8GU8uOqfSv1Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Crea una clase que implementa un árbol de decisión, utilice las funciones presentadas en clase, además incluya los siguientes hyperparámetros:\n",
        "   - max_depth: Cantidad máxima de variables que se pueden explorar\n",
        "   - min_split_samples: Cantidad mínima de muestras que deberá tener un nodo para poder ser dividido\n",
        "   - criterio: función que se utilizará para calcular la impuridad."
      ],
      "metadata": {
        "id": "Rld1szjGS1Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Decision Tree Class Creation\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion\n",
        "        self.root = None\n",
        "\n",
        "    def train(self, X, y, depth=0):\n",
        "        # Check stopping criteria\n",
        "        if depth == self.max_depth or len(X) < self.min_samples_split:\n",
        "            # Create a leaf node with the majority class or the average value\n",
        "            # depending on the problem (classification or regression)\n",
        "            # Example for classification:\n",
        "            value = most_common_class(y)\n",
        "            return Node(value=value)\n",
        "\n",
        "        # Choose the best feature and threshold to split the dataset\n",
        "        feature, threshold = find_best_split(X, y, self.criterion)\n",
        "\n",
        "        # Split the dataset into left and right subsets\n",
        "        X_left, y_left, X_right, y_right = split_dataset(\n",
        "            X, y, feature, threshold)\n",
        "\n",
        "        # Recursively build the sub-trees\n",
        "        left = self.train(X_left, y_left, depth=depth + 1)\n",
        "        right = self.train(X_right, y_right, depth=depth + 1)\n",
        "\n",
        "        # Create and return a decision node\n",
        "        return Node(feature=feature, threshold=threshold, left=left, right=right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Initialize an array to store the predictions\n",
        "        predictions = []\n",
        "\n",
        "        # Traverse the decision tree for each sample in X\n",
        "        for sample in X:\n",
        "            node = self.root\n",
        "            while node.left:\n",
        "                if sample[node.feature] <= node.threshold:\n",
        "                    node = node.left\n",
        "                else:\n",
        "                    node = node.right\n",
        "\n",
        "            # Append the predicted value for this sample\n",
        "            predictions.append(node.value)\n",
        "\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "ZFpFQw68S6D5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Divida los datos en los conjuntos tradicionales de entrenamiento y prueba, de forma manual, sin utilizar las utilidades de sklearn (puede utilizar índices de Numpy o Pandas)"
      ],
      "metadata": {
        "id": "F0NOzm20S41f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data Splitting\n",
        "# This function splits the dataset into training and test sets.\n",
        "def manual_train_test_split(X, y, train_proportion=0.8, random_state=None):\n",
        "    # Calculate the number of training samples\n",
        "    n_train_samples = int(train_proportion * len(X))\n",
        "\n",
        "    if random_state is not None:\n",
        "        # Set the seed for pseudo-random number generation\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    X_train, y_train = X[:n_train_samples], y[:n_train_samples]\n",
        "    X_test, y_test = X[n_train_samples:], y[n_train_samples:]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "8tymEVNtTFAI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Implemente una función que se llame `validacion_cruzada` que entrene $k$ modelos y reporte las métricas obtenidasd:\n",
        "  a. Divida el conjunto de entrenamiento en $k$ subconjuntos excluyentes\n",
        "  b. Para cada uno de los $k$ modelos, utilice un subconjunto como validación\n",
        "  c. Reporte la media y la desviación estándar para cada una de las métricas, todo debe realizarse solo usando Numpy:\n",
        "    - Accuracy\n",
        "    - Precision\n",
        "    - Recall\n",
        "    - F1"
      ],
      "metadata": {
        "id": "Z7uOzGT1TDui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Cross-Validation Implementation\n",
        "# This function performs cross-validation to evaluate model performance.\n",
        "def cross_validation(X, y, k=5, max_depth=None, min_samples_split=2, criterion='gini'):\n",
        "    # Split the training set into k subsets\n",
        "    subsets_X = np.array_split(X, k)\n",
        "    subsets_y = np.array_split(y, k)\n",
        "\n",
        "    # Lists to store metrics for each model\n",
        "    accuracy_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for i in range(k):\n",
        "        # Select the current validation set\n",
        "        X_valid = subsets_X[i]\n",
        "        y_valid = subsets_y[i]\n",
        "\n",
        "        # Create the training set excluding the validation set\n",
        "        X_train = np.concatenate([subsets_X[j] for j in range(k) if j != i])\n",
        "        y_train = np.concatenate([subsets_y[j] for j in range(k) if j != i])\n",
        "\n",
        "        # Train a decision tree model\n",
        "        tree = DecisionTree(\n",
        "            max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion)\n",
        "        tree.root = tree.train(X_train, y_train)\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        predictions = tree.predict(X_valid)\n",
        "\n",
        "        # Calculate metrics and record them\n",
        "        accuracy = accuracy_score(y_valid, predictions)\n",
        "        precision = precision_score(y_valid, predictions)\n",
        "        recall = recall_score(y_valid, predictions)\n",
        "        f1 = f1_score(y_valid, predictions)\n",
        "\n",
        "        accuracy_scores.append(accuracy)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    # Calculate the mean and standard deviation of the metrics\n",
        "    mean_accuracy = np.mean(accuracy_scores)\n",
        "    std_accuracy = np.std(accuracy_scores)\n",
        "    mean_precision = np.mean(precision_scores)\n",
        "    std_precision = np.std(precision_scores)\n",
        "    mean_recall = np.mean(recall_scores)\n",
        "    std_recall = np.std(recall_scores)\n",
        "    mean_f1 = np.mean(f1_scores)\n",
        "    std_f1 = np.std(f1_scores)\n",
        "\n",
        "    return {\n",
        "        \"mean_accuracy\": mean_accuracy,\n",
        "        \"std_accuracy\": std_accuracy,\n",
        "        \"mean_precision\": mean_precision,\n",
        "        \"std_precision\": std_precision,\n",
        "        \"mean_recall\": mean_recall,\n",
        "        \"std_recall\": std_recall,\n",
        "        \"mean_f1\": mean_f1,\n",
        "        \"std_f1\": std_f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "k05ZFJ1gTNk_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Entrene 10 combinaciones distintas de parámetros para su implementación de Arbol de Decisión y utilizando su implementación de `validacion_cruzada`.\n",
        "6. Utilizando los resultados obtenidos analice cuál y porqué es el mejor modelo para ser usado en producción.\n",
        "\n",
        "7. Compruebe las métricas usando el conjunto de prueba y analice el resultado"
      ],
      "metadata": {
        "id": "lvOOwLNKTMdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Ignorar warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Combinaciones\n",
        "param_combinations = [\n",
        "    {'max_depth': None, 'min_samples_split': 2, 'criterion': 'gini'},\n",
        "    {'max_depth': None, 'min_samples_split': 4, 'criterion': 'entropy'},\n",
        "    {'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'},\n",
        "    {'max_depth': 5, 'min_samples_split': 4, 'criterion': 'entropy'},\n",
        "    {'max_depth': 10, 'min_samples_split': 2, 'criterion': 'gini'},\n",
        "    {'max_depth': 10, 'min_samples_split': 4, 'criterion': 'entropy'},\n",
        "    {'max_depth': 15, 'min_samples_split': 2, 'criterion': 'gini'},\n",
        "    {'max_depth': 15, 'min_samples_split': 4, 'criterion': 'entropy'},\n",
        "    {'max_depth': 20, 'min_samples_split': 2, 'criterion': 'gini'},\n",
        "    {'max_depth': 20, 'min_samples_split': 4, 'criterion': 'entropy'}\n",
        "]\n",
        "\n",
        "# Lista para almacenar los resultados de las metricas\n",
        "results = []\n",
        "\n",
        "# Iterar sobre las combinaciones de parametros\n",
        "X = np.array([[2, 1],\n",
        "              [3, 2],\n",
        "              [4, 3],\n",
        "              [6, 4],\n",
        "              [7, 5],\n",
        "              [8, 6]])\n",
        "\n",
        "y = np.array([1, 0, 1, 1, 1, 0])\n",
        "\n",
        "X_train, y_train, X_test, y_test = manual_train_test_split(X, y, train_proportion=0.8, random_state=42)\n",
        "\n",
        "for params in param_combinations:\n",
        "    # Entrenar y evaluar el modelo utilizando validacion cruzada\n",
        "    metrics = cross_validation(X_train, y_train, k=5, **params)\n",
        "\n",
        "    # Agrega los resultados a la lista\n",
        "    results.append({\n",
        "        'params': params,\n",
        "        'metrics': metrics\n",
        "    })\n",
        "\n",
        "# Resultados 10 combinaciones - Primera mitad\n",
        "for idx, result in enumerate(results[:5]):\n",
        "    print(f\"Combinacion {idx + 1}:\")\n",
        "    print(f\"Parametros: {result['params']}\")\n",
        "    print(\"Metricas:\")\n",
        "    print(f\"\\t Punteria Media: {result['metrics']['mean_accuracy']}\")\n",
        "    print(f\"\\t Punteria Desviacion Estandar: {result['metrics']['std_accuracy']}\")\n",
        "    print(f\"\\t Precision Media: {result['metrics']['mean_precision']}\")\n",
        "    print(f\"\\t Precision Desviacion Estandar: {result['metrics']['std_precision']}\")\n",
        "    print(f\"\\t Recall Media: {result['metrics']['mean_recall']}\")\n",
        "    print(f\"\\t Recall Desviacion Estandar: {result['metrics']['std_recall']}\")\n",
        "    print(f\"\\t F1 Media: {result['metrics']['mean_f1']}\")\n",
        "    print(f\"\\t Desviacion Estandar F1: {result['metrics']['std_f1']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AcNDMuwRTrmi",
        "outputId": "cd877571-e731-47a4-91b2-5f5edc639d5f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combinacion 1:\n",
            "Parametros: {'max_depth': None, 'min_samples_split': 2, 'criterion': 'gini'}\n",
            "Metricas:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.4\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.4\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.4\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n",
            "Combinacion 2:\n",
            "Parametros: {'max_depth': None, 'min_samples_split': 4, 'criterion': 'entropy'}\n",
            "Metricas:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.6\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.6\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.6\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n",
            "Combinacion 3:\n",
            "Parametros: {'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'}\n",
            "Metricas:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.4\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.4\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.4\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n",
            "Combinacion 4:\n",
            "Parametros: {'max_depth': 5, 'min_samples_split': 4, 'criterion': 'entropy'}\n",
            "Metricas:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.6\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.6\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.6\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n",
            "Combinacion 5:\n",
            "Parametros: {'max_depth': 10, 'min_samples_split': 2, 'criterion': 'gini'}\n",
            "Metricas:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.4\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.4\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.4\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resultados 10 combinaciones - Segunda mitad\n",
        "for idx, result in enumerate(results[5:]):\n",
        "    print(f\"Combinacion {idx + 6}:\")\n",
        "    print(f\"Parametros: {result['params']}\")\n",
        "    print(\"Metricas:\")\n",
        "    print(f\"\\t Punteria Media: {result['metrics']['mean_accuracy']}\")\n",
        "    print(f\"\\t Punteria Desviacion Estandar: {result['metrics']['std_accuracy']}\")\n",
        "    print(f\"\\t Precision Media: {result['metrics']['mean_precision']}\")\n",
        "    print(f\"\\t Precision Desviacion Estandar: {result['metrics']['std_precision']}\")\n",
        "    print(f\"\\t Recall Media: {result['metrics']['mean_recall']}\")\n",
        "    print(f\"\\t Recall Desviacion Estandar: {result['metrics']['std_recall']}\")\n",
        "    print(f\"\\t F1 Media: {result['metrics']['mean_f1']}\")\n",
        "    print(f\"\\t Desviacion Estandar F1: {result['metrics']['std_f1']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qtQWlxzTYCQc",
        "outputId": "193bfd8b-d668-43a1-92dd-1fad3e9b43ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combinacion 6:\n",
            "Parametros: {'max_depth': 10, 'min_samples_split': 4, 'criterion': 'entropy'}\n",
            "Metricas:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.6\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.6\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.6\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n",
            "Combinacion 7:\n",
            "Parametros: {'max_depth': 15, 'min_samples_split': 2, 'criterion': 'gini'}\n",
            "Metricas:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.4\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.4\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.4\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n",
            "Combinacion 8:\n",
            "Parametros: {'max_depth': 15, 'min_samples_split': 4, 'criterion': 'entropy'}\n",
            "Metricas:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.6\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.6\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.6\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n",
            "Combinacion 9:\n",
            "Parametros: {'max_depth': 20, 'min_samples_split': 2, 'criterion': 'gini'}\n",
            "Metricas:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.4\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.4\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.4\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n",
            "Combinacion 10:\n",
            "Parametros: {'max_depth': 20, 'min_samples_split': 4, 'criterion': 'entropy'}\n",
            "Metricas:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.6\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.6\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.6\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analisis del modelo\n",
        "best_model_idx = np.argmax([result['metrics']['mean_accuracy'] for result in results])\n",
        "best_model_params = results[best_model_idx]['params']\n",
        "best_model_metrics = results[best_model_idx]['metrics']\n",
        "\n",
        "print(\"Mejor modelo:\")\n",
        "print(f\"Parametros: {best_model_params}\")\n",
        "print(\"Metricas en Cross-Validation:\")\n",
        "print(f\"\\t Punteria Media: {best_model_metrics['mean_accuracy']}\")\n",
        "print(f\"\\t Punteria Desviacion Estandar: {best_model_metrics['std_accuracy']}\")\n",
        "print(f\"\\t Precision Media: {best_model_metrics['mean_precision']}\")\n",
        "print(f\"\\t Precision Desviacion Estandar: {best_model_metrics['std_precision']}\")\n",
        "print(f\"\\t Recall Media: {best_model_metrics['mean_recall']}\")\n",
        "print(f\"\\t Recall Desviacion Estandar: {best_model_metrics['std_recall']}\")\n",
        "print(f\"\\t F1 Media: {best_model_metrics['mean_f1']}\")\n",
        "print(f\"\\t Desviacion Estandar F1: {best_model_metrics['std_f1']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NR9nRkEZVpj7",
        "outputId": "313c2449-2c80-4829-c374-62d60a5ee24f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor modelo:\n",
            "Parametros: {'max_depth': None, 'min_samples_split': 2, 'criterion': 'gini'}\n",
            "Metricas en Cross-Validation:\n",
            "\t Punteria Media: nan\n",
            "\t Punteria Desviacion Estandar: nan\n",
            "\t Precision Media: 0.4\n",
            "\t Precision Desviacion Estandar: 0.48989794855663565\n",
            "\t Recall Media: 0.4\n",
            "\t Recall Desviacion Estandar: 0.48989794855663565\n",
            "\t F1 Media: 0.4\n",
            "\t Desviacion Estandar F1: 0.48989794855663565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Análisis:\n",
        "Para los diferentes parámetros y los 2 criterios seleccionados respectivamente. Los resultados de media no cambian\n",
        "si se altera la profundidad y la cantidad de splits dentro de la muestra bajo los criterios seleccionadors. Por lo\n",
        "tanto, lo único que queda es verificar cuál de los modelos nos da mejores resultados. En este caso, el mejor modelo\n",
        "es el gini y ese es el que escogemos."
      ],
      "metadata": {
        "id": "P8HPWpbaVLYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Prueba en el set de prueba\n",
        "selected_model = DecisionTree(**best_model_params)\n",
        "selected_model.root = selected_model.train(X_train, y_train)\n",
        "\n",
        "test_predictions = selected_model.predict(X_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "test_precision = precision_score(y_test, test_predictions)\n",
        "test_recall = recall_score(y_test, test_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Metricas en el set de prueba:\")\n",
        "print(f\"Punteria: {test_accuracy}\")\n",
        "print(f\"Precision: {test_precision}\")\n",
        "print(f\"Recall: {test_recall}\")\n",
        "print(f\"F1 Score: {test_f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oFPNgVeJVI1E",
        "outputId": "7b4b2b0c-3966-4b31-93c1-4ad97abaca12"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metricas en el set de prueba:\n",
            "Punteria: 0.5\n",
            "Precision: 0.5\n",
            "Recall: 1.0\n",
            "F1 Score: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusiones:\n",
        "Con los resultados podemos darnos cuenta que el modelo seleccionado la verdad nos da\n",
        "medias aceptables para el set de datos que se hicieron. Con el mejor resultado gini al momento del\n",
        "split para nuestros datos. Por lo tanto, es válido decir que el modelo seleccionado es el que nos\n",
        "da mejores resultados."
      ],
      "metadata": {
        "id": "UqgGH0h1VX41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rúbrica para la Implementación de un Árbol de Decisión\n",
        "\n",
        "**Nota: Esta rúbrica se basa en la calidad de la implementación y los resultados obtenidos, no en la cantidad de código.**\n",
        "\n",
        "**1. Creación de la Clase Nodo (10 puntos)**\n",
        "\n",
        "- [ ] Se crea una clase `Nodo` con los atributos mencionados en las especificaciones (feature, umbral, gini, cantidad_muestras, valor, izquierda, derecha).\n",
        "- [ ] Los atributos se definen correctamente y se asignan de manera apropiada.\n",
        "\n",
        "**2. Creación de la Clase Árbol de Decisión (20 puntos)**\n",
        "\n",
        "- [ ] Se crea una clase que implementa un árbol de decisión.\n",
        "- [ ] La clase utiliza las funciones presentadas en el cuaderno.\n",
        "- [ ] Se implementan los hyperparámetros solicitados (max_depth, min_split_samples, criterio).\n",
        "- [ ] La clase es capaz de entrenar un árbol de decisión con los hyperparámetros especificados.\n",
        "\n",
        "**3. División de Datos (10 puntos)**\n",
        "\n",
        "- [ ] Los datos se dividen en conjuntos de entrenamiento y prueba de forma manual.\n",
        "- [ ] Se utiliza Numpy o Pandas para realizar esta división.\n",
        "- [ ] Se garantiza que los conjuntos sean excluyentes.\n",
        "\n",
        "**4. Implementación de Validación Cruzada (20 puntos)**\n",
        "\n",
        "- [ ] Se implementa la función `validacion_cruzada` correctamente.\n",
        "- [ ] Los datos de entrenamiento se dividen en k subconjuntos excluyentes.\n",
        "- [ ] Se entrena y evalúa un modelo para cada subconjunto de validación.\n",
        "- [ ] Se calculan y reportan las métricas de accuracy, precision, recall y F1.\n",
        "- [ ] Se calcula la media y la desviación estándar de estas métricas.\n",
        "\n",
        "**5. Entrenamiento de Modelos (20 puntos)**\n",
        "\n",
        "- [ ] Se entrenan 10 combinaciones distintas de parámetros para el árbol de decisión.\n",
        "- [ ] Cada combinación se entrena utilizando la función `validacion_cruzada`.\n",
        "- [ ] Los resultados de las métricas se registran adecuadamente.\n",
        "\n",
        "**6. Análisis de Modelos (10 puntos)**\n",
        "\n",
        "- [ ] Se analizan los resultados obtenidos y se selecciona el mejor modelo para ser utilizado en producción.\n",
        "- [ ] Se proporciona una justificación clara y fundamentada sobre por qué se eligió ese modelo.\n",
        "\n",
        "**7. Prueba en el Conjunto de Prueba (10 puntos)**\n",
        "\n",
        "- [ ] Se comprueban las métricas del modelo seleccionado utilizando el conjunto de prueba.\n",
        "- [ ] Se analizan los resultados y se comentan las conclusiones.\n",
        "\n",
        "**General (10 puntos)**\n",
        "\n",
        "- [ ] El código se documenta de manera adecuada, incluyendo comentarios que expliquen las secciones clave.\n",
        "- [ ] El código se ejecuta sin errores y sigue buenas prácticas de programación.\n",
        "- [ ] La presentación de los resultados es clara y fácil de entender.\n",
        "- [ ] Se cumple con todos los requisitos y las especificaciones proporcionadas.\n",
        "\n",
        "**Puntuación Total: 100 puntos**\n",
        "\n"
      ],
      "metadata": {
        "id": "i_9m6Jqr25fi"
      }
    }
  ]
}